{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56bddff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is our code for the ML Project in EENG 680\n",
    "#datasets have been uploaded with 400 samples per folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "daaf770c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.12.0.88-cp37-abi3-macosx_13_0_arm64.whl.metadata (19 kB)\n",
      "Collecting numpy<2.3.0,>=2 (from opencv-python)\n",
      "  Downloading numpy-2.2.6-cp313-cp313-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Downloading opencv_python-4.12.0.88-cp37-abi3-macosx_13_0_arm64.whl (37.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.9/37.9 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m  \u001b[33m0:00:27\u001b[0mm0:00:01\u001b[0m00:02\u001b[0mm\n",
      "\u001b[?25hDownloading numpy-2.2.6-cp313-cp313-macosx_14_0_arm64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m961.2 kB/s\u001b[0m  \u001b[33m0:00:05\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy, opencv-python\n",
      "\u001b[2K  Attempting uninstall: numpy\n",
      "\u001b[2K    Found existing installation: numpy 2.3.4\n",
      "\u001b[2K    Uninstalling numpy-2.3.4:\n",
      "\u001b[2K      Successfully uninstalled numpy-2.3.4\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [opencv-python]0m [opencv-python]\n",
      "\u001b[1A\u001b[2KSuccessfully installed numpy-2.2.6 opencv-python-4.12.0.88\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b7ba26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 combined_image_path  label\n",
      "0  Combined_Dataset/closed_eye_no_yawn/sample_000...      2\n",
      "1  Combined_Dataset/closed_eye_no_yawn/sample_000...      2\n",
      "2  Combined_Dataset/closed_eye_no_yawn/sample_000...      2\n",
      "3  Combined_Dataset/closed_eye_no_yawn/sample_000...      2\n",
      "4  Combined_Dataset/closed_eye_no_yawn/sample_000...      2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "#making dataframes for the mixed dataset folders to create random pairs of eye and mouth (yawn state) samples so the training model reads them as one sample\n",
    "#closed eye and no yawn\n",
    "\n",
    "closed_eye_dir = \"Merged_Datasets/closed_eye_no_yawn/closed_eye\"\n",
    "no_yawn_dir =  \"Merged_Datasets/closed_eye_no_yawn/no_yawn\"\n",
    "\n",
    "#output folder to combinie the images into 1 for easier processing\n",
    "output_dir = \"Combined_Dataset/closed_eye_no_yawn\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "random_seed = 42\n",
    "pairing_mode = \"random\"\n",
    "\n",
    "random.seed(random_seed)\n",
    "\n",
    "images_closed_eye = [os.path.abspath(os.path.join(closed_eye_dir, f)) for f in os.listdir(closed_eye_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "images_no_yawn = [os.path.abspath(os.path.join(no_yawn_dir, f)) for f in os.listdir(no_yawn_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "#sort for reproducibiltiy\n",
    "images_closed_eye.sort()\n",
    "images_no_yawn.sort()\n",
    "\n",
    "random.shuffle(images_closed_eye)\n",
    "random.shuffle(images_no_yawn)\n",
    "\n",
    "min_len = min(len(images_closed_eye), len(images_no_yawn))\n",
    "pairs = list(zip(images_closed_eye[:min_len], images_no_yawn[:min_len]))\n",
    "\n",
    "csv_rows = []\n",
    "\n",
    "for idx, (eye_path, mouth_path) in enumerate(pairs):\n",
    "\n",
    "    #read images\n",
    "    eye = cv2.imread(eye_path)\n",
    "    mouth = cv2.imread(mouth_path)\n",
    "\n",
    "    #skip missing images \n",
    "    if eye is None or mouth is None:\n",
    "        continue\n",
    "\n",
    "    #resize images to 128x128\n",
    "    eye = cv2.resize(eye, (128, 128))\n",
    "    mouth = cv2.resize(mouth, (128, 128))\n",
    "\n",
    "    #combine the images side by side\n",
    "    combined = cv2.hconcat([eye, mouth])\n",
    "\n",
    "    #save combined image\n",
    "    save_path = os.path.join(output_dir, f\"sample_{idx:04d}.jpg\")\n",
    "    cv2.imwrite(save_path, combined)\n",
    "\n",
    "    # Add row to CSV\n",
    "    csv_rows.append([save_path, 2])   # label 2 = closed eyes and no yawn - more drowsy state\n",
    "\n",
    "# Save CSV\n",
    "df_closed_no_yawn = pd.DataFrame(csv_rows, columns=['combined_image_path', 'label'])\n",
    "df_closed_no_yawn.to_csv(\"combined_closed_eye_no_yawn.csv\", index=False)\n",
    "\n",
    "print(df_closed_no_yawn.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6aeeebd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/leilatawfik/Desktop/FALL 2025/ENGG680_ML_Project/Merged_Datasets/closed_eye_no_yawn/no_yawn/2837.jpg\n",
      "True\n",
      "Current working dir: /Users/leilatawfik/Desktop/FALL 2025/ENGG680_ML_Project\n"
     ]
    }
   ],
   "source": [
    "#trying to see what the paths look like and if they actually exist cause i cannot access the images through the created .csv\n",
    "test_df = pd.read_csv(\"paired_closed_eye_no_yawn.csv\")\n",
    "#print(test_df.head())\n",
    "\n",
    "#print(test_df['images_closed_eye_path'][0])\n",
    "#print(test_df['images_no_yawn_path'][0])\n",
    "\n",
    "test_path = test_df['images_no_yawn_path'][0]\n",
    "print(test_path)\n",
    "print(os.path.exists(test_path))\n",
    "\n",
    "print(\"Current working dir:\", os.getcwd())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5fdb53a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The absolute path is: /Users/leilatawfik/Desktop/FALL 2025/ENGG680_ML_Project/C:/Users/saumy/Desktop/ENGG680_ML_Project/Merged_Datasets/closed_eye_no_yawn/no_yawn/2837.jpg\n"
     ]
    }
   ],
   "source": [
    "#testing to see if the absolute path command works\n",
    "\n",
    "relative_path = \"C:/Users/saumy/Desktop/ENGG680_ML_Project/Merged_Datasets/closed_eye_no_yawn/no_yawn/2837.jpg\"\n",
    "absolute_path = os.path.abspath(relative_path)\n",
    "\n",
    "print(f\"The absolute path is: {absolute_path}\")\n",
    "\n",
    "#i am so confused - i am assuming it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cd1634b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                combined_image_path  label\n",
      "0  Combined_Dataset/closed_eye_yawn/sample_0000.jpg      3\n",
      "1  Combined_Dataset/closed_eye_yawn/sample_0001.jpg      3\n",
      "2  Combined_Dataset/closed_eye_yawn/sample_0002.jpg      3\n",
      "3  Combined_Dataset/closed_eye_yawn/sample_0003.jpg      3\n",
      "4  Combined_Dataset/closed_eye_yawn/sample_0004.jpg      3\n"
     ]
    }
   ],
   "source": [
    "#making dataframes for the mixed dataset folders to create random pairs of eye and mouth (yawn state) samples so the training model reads them as one sample\n",
    "#closed eye and yawn\n",
    "\n",
    "closed_eye_dir2 = \"Merged_Datasets/closed_eye_yawn/closed_eye\"\n",
    "yawn_dir =  \"Merged_Datasets/closed_eye_yawn/yawn\"\n",
    "random_seed = 42\n",
    "pairing_mode = \"random\"\n",
    "\n",
    "random.seed(random_seed)\n",
    "\n",
    "images_closed_eye2 = [os.path.abspath(os.path.join(closed_eye_dir2, f)) for f in os.listdir(closed_eye_dir2) if f.lower().endswith(('.jpg', '.png'))]\n",
    "images_yawn = [os.path.abspath(os.path.join(yawn_dir, f)) for f in os.listdir(yawn_dir) if f.lower().endswith(('.jpg', '.png'))]\n",
    "\n",
    "#output folder to combinie the images into 1 for easier processing\n",
    "output_dir = \"Combined_Dataset/closed_eye_yawn\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "#sort for reproducibiltiy\n",
    "images_closed_eye2.sort()\n",
    "images_yawn.sort()\n",
    "\n",
    "random.shuffle(images_closed_eye2)\n",
    "random.shuffle(images_yawn)\n",
    "\n",
    "\n",
    "min_len = min(len(images_closed_eye2), len(images_yawn))\n",
    "pairs = list(zip(images_closed_eye2[:min_len], images_yawn[:min_len]))\n",
    "\n",
    "csv_rows = []\n",
    "\n",
    "for idx, (eye_path, mouth_path) in enumerate(pairs):\n",
    "\n",
    "    #read images\n",
    "    eye = cv2.imread(eye_path)\n",
    "    mouth = cv2.imread(mouth_path)\n",
    "\n",
    "    #skip missing images \n",
    "    if eye is None or mouth is None:\n",
    "        continue\n",
    "\n",
    "    #resize images to 128x128\n",
    "    eye = cv2.resize(eye, (128, 128))\n",
    "    mouth = cv2.resize(mouth, (128, 128))\n",
    "\n",
    "    #combine the images side by side\n",
    "    combined = cv2.hconcat([eye, mouth])\n",
    "\n",
    "    #save combined image\n",
    "    save_path = os.path.join(output_dir, f\"sample_{idx:04d}.jpg\")\n",
    "    cv2.imwrite(save_path, combined)\n",
    "\n",
    "    # Add row to CSV\n",
    "    csv_rows.append([save_path, 3])   # label 3 = closed eyes and yawn - most drowsy state\n",
    "\n",
    "# Save CSV\n",
    "df_closed_yawn = pd.DataFrame(csv_rows, columns=['combined_image_path', 'label'])\n",
    "df_closed_yawn.to_csv(\"combined_closed_eye_yawn.csv\", index=False)\n",
    "\n",
    "print(df_closed_yawn.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d123d208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 combined_image_path  label\n",
      "0  Combined_Dataset/open_eye_no_yawn/sample_0000.jpg      0\n",
      "1  Combined_Dataset/open_eye_no_yawn/sample_0001.jpg      0\n",
      "2  Combined_Dataset/open_eye_no_yawn/sample_0002.jpg      0\n",
      "3  Combined_Dataset/open_eye_no_yawn/sample_0003.jpg      0\n",
      "4  Combined_Dataset/open_eye_no_yawn/sample_0004.jpg      0\n"
     ]
    }
   ],
   "source": [
    "#making dataframes for the mixed dataset folders to create random pairs of eye and mouth (yawn state) samples so the training model reads them as one sample\n",
    "#open eye and no yawning\n",
    "\n",
    "open_eye_dir = \"Merged_Datasets/open_eye_no_yawn/open_eye\"\n",
    "no_yawn_dir2 =  \"Merged_Datasets/open_eye_no_yawn/no_yawn\"\n",
    "random_seed = 42\n",
    "pairing_mode = \"random\"\n",
    "\n",
    "random.seed(random_seed)\n",
    "\n",
    "images_open_eye = [os.path.abspath(os.path.join(open_eye_dir, f)) for f in os.listdir(open_eye_dir) if f.lower().endswith(('.jpg', '.png'))]\n",
    "images_no_yawn2 = [os.path.abspath(os.path.join(no_yawn_dir2, f)) for f in os.listdir(no_yawn_dir2) if f.lower().endswith(('.jpg', '.png'))]\n",
    "\n",
    "#sort for reproducibiltiy\n",
    "images_open_eye.sort()\n",
    "images_no_yawn2.sort()\n",
    "\n",
    "#output folder to combinie the images into 1 for easier processing\n",
    "output_dir = \"Combined_Dataset/open_eye_no_yawn\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "random.shuffle(images_open_eye)\n",
    "random.shuffle(images_no_yawn2)\n",
    "\n",
    "min_len = min(len(images_open_eye), len(images_no_yawn2))\n",
    "pairs = list(zip(images_open_eye[:min_len], images_no_yawn2[:min_len]))\n",
    "\n",
    "csv_rows = []\n",
    "\n",
    "for idx, (eye_path, mouth_path) in enumerate(pairs):\n",
    "\n",
    "    #read images\n",
    "    eye = cv2.imread(eye_path)\n",
    "    mouth = cv2.imread(mouth_path)\n",
    "\n",
    "    #skip missing images \n",
    "    if eye is None or mouth is None:\n",
    "        continue\n",
    "\n",
    "    #resize images to 128x128\n",
    "    eye = cv2.resize(eye, (128, 128))\n",
    "    mouth = cv2.resize(mouth, (128, 128))\n",
    "\n",
    "    #combine the images side by side\n",
    "    combined = cv2.hconcat([eye, mouth])\n",
    "\n",
    "    #save combined image\n",
    "    save_path = os.path.join(output_dir, f\"sample_{idx:04d}.jpg\")\n",
    "    cv2.imwrite(save_path, combined)\n",
    "\n",
    "    # Add row to CSV\n",
    "    csv_rows.append([save_path, 0])   # label 0 = indicates open eye and no yawn - alert state\n",
    "\n",
    "# Save CSV\n",
    "df_open_eye_no_yawn = pd.DataFrame(csv_rows, columns=['combined_image_path', 'label'])\n",
    "df_open_eye_no_yawn.to_csv(\"combined_closed_eye_yawn.csv\", index=False)\n",
    "\n",
    "print(df_open_eye_no_yawn.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ce0bd6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              combined_image_path  label\n",
      "0  Combined_Dataset/open_eye_yawn/sample_0000.jpg      1\n",
      "1  Combined_Dataset/open_eye_yawn/sample_0001.jpg      1\n",
      "2  Combined_Dataset/open_eye_yawn/sample_0002.jpg      1\n",
      "3  Combined_Dataset/open_eye_yawn/sample_0003.jpg      1\n",
      "4  Combined_Dataset/open_eye_yawn/sample_0004.jpg      1\n"
     ]
    }
   ],
   "source": [
    "#making dataframes for the mixed dataset folders to create random pairs of eye and mouth (yawn state) samples so the training model reads them as one sample\n",
    "#open eye and yawning\n",
    "\n",
    "open_eye_dir2 = \"Merged_Datasets/open_eye_yawn/open_eye\"\n",
    "yawn_dir2 =  \"Merged_Datasets/open_eye_yawn/yawn\"\n",
    "random_seed = 42\n",
    "pairing_mode = \"random\"\n",
    "\n",
    "random.seed(random_seed)\n",
    "\n",
    "images_open_eye2 = [os.path.abspath(os.path.join(open_eye_dir2, f)) for f in os.listdir(open_eye_dir2) if f.lower().endswith(('.jpg', '.png'))]\n",
    "images_yawn2 = [os.path.abspath(os.path.join(yawn_dir2, f)) for f in os.listdir(yawn_dir2) if f.lower().endswith(('.jpg', '.png'))]\n",
    "\n",
    "#sort for reproducibiltiy\n",
    "images_open_eye2.sort()\n",
    "images_yawn2.sort()\n",
    "\n",
    "#output folder to combinie the images into 1 for easier processing\n",
    "output_dir = \"Combined_Dataset/open_eye_yawn\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "random.shuffle(images_open_eye2)\n",
    "random.shuffle(images_yawn2)\n",
    "\n",
    "\n",
    "min_len = min(len(images_open_eye2), len(images_yawn2))\n",
    "pairs = list(zip(images_open_eye2[:min_len], images_yawn2[:min_len]))\n",
    "\n",
    "csv_rows = []\n",
    "\n",
    "for idx, (eye_path, mouth_path) in enumerate(pairs):\n",
    "\n",
    "    #read images\n",
    "    eye = cv2.imread(eye_path)\n",
    "    mouth = cv2.imread(mouth_path)\n",
    "\n",
    "    #skip missing images \n",
    "    if eye is None or mouth is None:\n",
    "        continue\n",
    "\n",
    "    #resize images to 128x128\n",
    "    eye = cv2.resize(eye, (128, 128))\n",
    "    mouth = cv2.resize(mouth, (128, 128))\n",
    "\n",
    "    #combine the images side by side\n",
    "    combined = cv2.hconcat([eye, mouth])\n",
    "\n",
    "    #save combined image\n",
    "    save_path = os.path.join(output_dir, f\"sample_{idx:04d}.jpg\")\n",
    "    cv2.imwrite(save_path, combined)\n",
    "\n",
    "    # Add row to CSV\n",
    "    csv_rows.append([save_path, 1])   # label 1 = indicates open eye and yawn - somewhat drowsy state\n",
    "\n",
    "# Save CSV\n",
    "df_open_yawn = pd.DataFrame(csv_rows, columns=['combined_image_path', 'label'])\n",
    "df_open_yawn.to_csv(\"combined_closed_eye_yawn.csv\", index=False)\n",
    "\n",
    "print(df_open_yawn.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ebc5fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   combined_image_path  label\n",
      "0    Combined_Dataset/closed_eye_no_yawn/sample_009...      2\n",
      "1    Combined_Dataset/closed_eye_no_yawn/sample_006...      2\n",
      "2       Combined_Dataset/open_eye_yawn/sample_0063.jpg      1\n",
      "3     Combined_Dataset/closed_eye_yawn/sample_0133.jpg      3\n",
      "4       Combined_Dataset/open_eye_yawn/sample_0066.jpg      1\n",
      "..                                                 ...    ...\n",
      "795     Combined_Dataset/open_eye_yawn/sample_0071.jpg      1\n",
      "796     Combined_Dataset/open_eye_yawn/sample_0106.jpg      1\n",
      "797  Combined_Dataset/open_eye_no_yawn/sample_0070.jpg      0\n",
      "798   Combined_Dataset/closed_eye_yawn/sample_0035.jpg      3\n",
      "799     Combined_Dataset/open_eye_yawn/sample_0102.jpg      1\n",
      "\n",
      "[800 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#add all dataframes together to create one full dataset\n",
    "full_set_df = pd.concat([df_open_yawn, df_open_eye_no_yawn, df_closed_yawn, df_closed_no_yawn], ignore_index=True)\n",
    "\n",
    "#randomly shuffle the full dataset\n",
    "full_set_df = full_set_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "print(full_set_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "df508ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.7.2-cp313-cp313-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.22.0 in ./.venv/lib/python3.13/site-packages (from scikit-learn) (2.3.4)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Using cached scipy-1.16.3-cp313-cp313-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Using cached scikit_learn-1.7.2-cp313-cp313-macosx_12_0_arm64.whl (8.6 MB)\n",
      "Using cached joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Using cached scipy-1.16.3-cp313-cp313-macosx_14_0_arm64.whl (20.9 MB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [scikit-learn][0m [scikit-learn]\n",
      "\u001b[1A\u001b[2KSuccessfully installed joblib-1.5.2 scikit-learn-1.7.2 scipy-1.16.3 threadpoolctl-3.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b398b0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt     \n",
    "from sklearn.model_selection import train_test_split    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e32ae76b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">254</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">127</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">125</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">119040</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │    <span style=\"color: #00af00; text-decoration-color: #00af00\">15,237,248</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">516</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m254\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_6 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m127\u001b[0m, \u001b[38;5;34m32\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_7 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m125\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_7 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_3 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m119040\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │    \u001b[38;5;34m15,237,248\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │           \u001b[38;5;34m516\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">15,257,156</span> (58.20 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m15,257,156\u001b[0m (58.20 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">15,257,156</span> (58.20 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m15,257,156\u001b[0m (58.20 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid dtype: object",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(model.summary())\n\u001b[32m     28\u001b[39m train_labels = train_df[\u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m].values\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/FALL 2025/ENGG680_ML_Project/.venv/lib/python3.13/site-packages/keras/src/utils/traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/FALL 2025/ENGG680_ML_Project/.venv/lib/python3.13/site-packages/optree/ops.py:766\u001b[39m, in \u001b[36mtree_map\u001b[39m\u001b[34m(func, tree, is_leaf, none_is_leaf, namespace, *rests)\u001b[39m\n\u001b[32m    764\u001b[39m leaves, treespec = _C.flatten(tree, is_leaf, none_is_leaf, namespace)\n\u001b[32m    765\u001b[39m flat_args = [leaves] + [treespec.flatten_up_to(r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rests]\n\u001b[32m--> \u001b[39m\u001b[32m766\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtreespec\u001b[49m\u001b[43m.\u001b[49m\u001b[43munflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mflat_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mValueError\u001b[39m: Invalid dtype: object"
     ]
    }
   ],
   "source": [
    "#CNN model used for image classification\n",
    "\n",
    "#test train split, 20% test size\n",
    "\n",
    "train_df, test_df = train_test_split(\n",
    "    full_set_df, \n",
    "    test_size=0.2,\n",
    "    stratify=full_set_df[\"label\"], #stratify to maintain class distribution\n",
    "    random_state=42 #for reproducibility\n",
    ")\n",
    "\n",
    "#cnn model architecture\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=(128, 256, 3)),  #input shape is 128x256x3 (height, width, channels)\n",
    "    layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),        \n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(4, activation='softmax')  #4 classes for classification\n",
    "])  \n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])         \n",
    "print(model.summary())\n",
    "\n",
    "train_labels = train_df['label'].values\n",
    "model.fit(train_df, train_labels, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c3d961",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
